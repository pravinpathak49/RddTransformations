{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark_SQL').config('spark.sql.shuffle.partitions',2).getOrCreate()\n",
    "#read the csv file in dataframe\n",
    "OrderSchema = StructType([StructField(\"order_id\", IntegerType()),StructField(\"order_date\", StringType()), StructField(\"customer_id\", IntegerType()),StructField(\"order_status\", StringType())])\n",
    "orderDF = spark.read.csv(path='/home/hduser/Documents/RddTransformations/Data/orders.csv', sep=\",\", schema=OrderSchema)\n",
    "\n",
    "orderItemschema = StructType([StructField(\"order_item_id\", IntegerType()),StructField(\"order_item_order_id\", IntegerType()),StructField(\"order_item_product_id\", IntegerType()),StructField(\"order_item_quantity\", IntegerType()),StructField(\"order_item_subtotal\", FloatType()), StructField(\"order_item_product_price\", FloatType())])\n",
    "orderItemsDF = spark.read.csv(path='/home/hduser/Documents/RddTransformations/Data/order_items.csv', sep=\",\", schema=orderItemschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF.registerTempTable('orders')\n",
    "orderItemsDF.registerTempTable('order_items')\n",
    "\n",
    "#to Drop temp table\n",
    "#spark.catalog.dropTempView('order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |order_items|       true|\n",
      "|        |     orders|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql whatever it executes it returns DataFrame\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from orders').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the order_items whose order_status is complete or closed\n",
    "#1. using in operator\n",
    " spark.sql('select * from order_items where order_item_order_id in \\\n",
    " (select order_id from orders where order_status in (\"COMPLETE\",\"CLOSED\"))').show(5)\n",
    "#2 Using Join \n",
    "#spark.sql('select * from order_items join orders on \\\n",
    "#order_items.order_item_order_id = orders.order_id where orders.order_status in (\"COMPLETE\",\"CLOSED\")').show()\n",
    "\n",
    "#3 Using ASCII standard sql code (suggested)\n",
    "#spark.sql('select * from order_items, orders where order_items.order_item_order_id = orders.order_id \\\n",
    "#and orders.order_status in (\"COMPLETE\",\"CLOSED\")').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11452"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show all the orders whose order_id  is not in order_items\n",
    "#spark.sql('select * from orders where order_id not in (select order_item_order_id from order_items)').count()\n",
    "#11452\n",
    "#using Join\n",
    "spark.sql('select * from orders left outer join order_items on\\\n",
    "    orders.order_id = order_items.order_item_order_id where order_items.order_item_order_id is null').count()\n",
    "#11452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+------------------------+\n",
      "|         order_date|order_item_product_id|sum(order_item_subtotal)|\n",
      "+-------------------+---------------------+------------------------+\n",
      "|2013-07-25 00:00:00|                 1014|       6397.439922332764|\n",
      "|2013-07-25 00:00:00|                  926|       79.94999694824219|\n",
      "|2013-07-25 00:00:00|                  191|        8499.15005493164|\n",
      "|2013-07-25 00:00:00|                  134|                   100.0|\n",
      "|2013-07-25 00:00:00|                  276|       255.9199981689453|\n",
      "+-------------------+---------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the revenue per day per product item\n",
    "spark.sql('select order_date, order_item_product_id,sum(order_item_subtotal) from orders join order_items on \\\n",
    "          orders.order_id = order_items.order_item_order_id group by order_date,order_item_product_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('select order_date, order_item_product_id,sum(order_item_subtotal) from orders join order_items on \\\n",
    "#          orders.order_id = order_items.order_item_order_id group by order_date,order_item_product_id')\\\n",
    "#          .write.csv('/home/hduser/Documents/RddTransformations/sparksql/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_subtotal)|\n",
      "+-------------------+------------------------+\n",
      "|                  2|       579.9800109863281|\n",
      "|                  4|       699.8500099182129|\n",
      "|                  5|      1129.8600387573242|\n",
      "|                 10|        651.920015335083|\n",
      "|                 12|      1299.8700256347656|\n",
      "+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using group by functions. We can have those columns in select by clause which we are doing group by.\n",
    "spark.sql('select order_item_order_id, sum(order_item_subtotal) from order_items\\\n",
    "           group by order_item_order_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|                  1|                  957|             299.98|299.9800109863281|\n",
      "|                  2|                 1073|             199.99|579.9800109863281|\n",
      "|                  2|                  502|              250.0|579.9800109863281|\n",
      "|                  2|                  403|             129.99|579.9800109863281|\n",
      "|                  4|                  897|              49.98|699.8500099182129|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if you need more columns you need to self join and in this situation partition by comes\n",
    "spark.sql('select ot.order_item_order_id,ot.order_item_product_id ,ot.order_item_subtotal,o.revenue \\\n",
    "           from order_items ot join \\\n",
    "          (select order_item_order_id, sum(order_item_subtotal)as revenue \\\n",
    "          from order_items group by order_item_order_id)o\\\n",
    "          on ot.order_item_order_id = o.order_item_order_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|                  2|                 1073|             199.99|579.9800109863281|\n",
      "|                  2|                  502|              250.0|579.9800109863281|\n",
      "|                  2|                  403|             129.99|579.9800109863281|\n",
      "|                  4|                  897|              49.98|699.8500099182129|\n",
      "|                  4|                  365|             299.95|699.8500099182129|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using partition By we can get multiple columns value as well. Not needed to join and exeution is also fast\n",
    "#get the total revenue as per product id.\n",
    "spark.sql('select order_item_order_id,order_item_product_id ,order_item_subtotal ,\\\n",
    "sum(order_item_subtotal) over (partition by order_item_order_id)as revenue from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|Percentage|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "|                  2|                 1073|             199.99|579.9800109863281|     34.48|\n",
      "|                  2|                  502|              250.0|579.9800109863281|      43.1|\n",
      "|                  2|                  403|             129.99|579.9800109863281|     22.41|\n",
      "|                  4|                  897|              49.98|699.8500099182129|      7.14|\n",
      "|                  4|                  365|             299.95|699.8500099182129|     42.86|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the % of each product id\n",
    "spark.sql('select order_item_order_id,order_item_product_id ,order_item_subtotal ,\\\n",
    "sum(order_item_subtotal) over (partition by order_item_order_id)as revenue,\\\n",
    "round(order_item_subtotal/(sum(order_item_subtotal) over (partition by order_item_order_id))* 100 ,2)\\\n",
    "as Percentage from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|Percentage|rank|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "|                  2|                  403|             129.99|579.9800109863281|      0.22|   1|\n",
      "|                  2|                 1073|             199.99|579.9800109863281|      0.34|   2|\n",
      "|                  2|                  502|              250.0|579.9800109863281|      0.43|   3|\n",
      "|                  4|                  897|              49.98|699.8500099182129|      0.07|   1|\n",
      "|                  4|                  502|              150.0|699.8500099182129|      0.21|   2|\n",
      "|                  4|                 1014|             199.92|699.8500099182129|      0.29|   3|\n",
      "|                  4|                  365|             299.95|699.8500099182129|      0.43|   4|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank the records based on order_item_subtotal of each order_id.\n",
    "#these analytical functions can be used in select clause.\n",
    "#execution of sql happens like\n",
    "#from --> where and/or join --> group by --> having --> select --> order by\n",
    "spark.sql('select order_item_order_id,\\\n",
    "order_item_product_id,\\\n",
    "order_item_subtotal,\\\n",
    "sum(order_item_subtotal) over(partition by order_item_order_id) as revenue,\\\n",
    "round(order_item_subtotal/sum(order_item_subtotal) over(partition by order_item_order_id), 2)as Percentage, \\\n",
    "rank() over (partition by order_item_order_id order by order_item_subtotal) as rank \\\n",
    "from order_items').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+-------+\n",
      "|order_item_order_id|order_item_subtotal|revenue|ranking|\n",
      "+-------------------+-------------------+-------+-------+\n",
      "|                  2|             129.99| 579.98|      1|\n",
      "|                  2|             199.99| 579.98|      2|\n",
      "|                  4|              49.98| 699.85|      1|\n",
      "|                  4|              150.0| 699.85|      2|\n",
      "|                  5|              99.96|1129.86|      1|\n",
      "|                  5|             129.99|1129.86|      2|\n",
      "|                 10|              21.99| 651.92|      1|\n",
      "|                 10|              99.96| 651.92|      2|\n",
      "|                 12|              100.0|1299.87|      1|\n",
      "|                 12|             149.94|1299.87|      2|\n",
      "|                 13|             127.96| 127.96|      1|\n",
      "|                 14|               50.0| 549.94|      1|\n",
      "|                 14|              99.96| 549.94|      2|\n",
      "|                 18|             119.98| 449.96|      1|\n",
      "|                 18|             129.99| 449.96|      2|\n",
      "|                 25|             399.98| 399.98|      1|\n",
      "|                 28|              59.99| 1159.9|      1|\n",
      "|                 28|              99.99| 1159.9|      2|\n",
      "|                 31|             499.95| 499.95|      1|\n",
      "|                 36|              100.0| 799.96|      1|\n",
      "+-------------------+-------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get top 2 items of each order_id\n",
    "spark.sql('select  *from (select \\\n",
    "order_item_order_id, \\\n",
    "order_item_subtotal, \\\n",
    "round(sum(order_item_subtotal) over (partition by order_item_order_id), 2) as revenue, \\\n",
    "rank() over(partition by order_item_order_id order by order_item_subtotal) as ranking \\\n",
    "from order_items ) where ranking < 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------+-------+\n",
      "|order_item_order_id|          revenue|order_item_subtotal|leading|\n",
      "+-------------------+-----------------+-------------------+-------+\n",
      "|                  2|579.9800109863281|              250.0| 199.99|\n",
      "|                  2|579.9800109863281|             199.99| 129.99|\n",
      "|                  2|579.9800109863281|             129.99|   null|\n",
      "|                  4|699.8500099182129|             299.95| 199.92|\n",
      "|                  4|699.8500099182129|             199.92|  150.0|\n",
      "+-------------------+-----------------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we use lead() and lag() to get the next or previous item for the column specified in the functions.\n",
    "spark.sql('select order_item_order_id,\\\n",
    "sum(order_item_subtotal) over(partition by order_item_order_id) as revenue,\\\n",
    "order_item_subtotal,\\\n",
    "lead(order_item_subtotal) over (partition by order_item_order_id order by order_item_subtotal desc) as leading \\\n",
    "from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------------+-------+\n",
      "|order_item_order_id|revenue|order_item_subtotal|lagging|\n",
      "+-------------------+-------+-------------------+-------+\n",
      "|                  2| 579.98|              250.0|   null|\n",
      "|                  2| 579.98|             199.99|  250.0|\n",
      "|                  2| 579.98|             129.99| 199.99|\n",
      "|                  4| 699.85|             299.95|   null|\n",
      "|                  4| 699.85|             199.92| 299.95|\n",
      "|                  4| 699.85|              150.0| 199.92|\n",
      "|                  4| 699.85|              49.98|  150.0|\n",
      "|                  5|1129.86|             299.98|   null|\n",
      "|                  5|1129.86|             299.98| 299.98|\n",
      "|                  5|1129.86|             299.95| 299.98|\n",
      "|                  5|1129.86|             129.99| 299.95|\n",
      "|                  5|1129.86|              99.96| 129.99|\n",
      "|                 10| 651.92|             199.99|   null|\n",
      "|                 10| 651.92|             199.99| 199.99|\n",
      "|                 10| 651.92|             129.99| 199.99|\n",
      "|                 10| 651.92|              99.96| 129.99|\n",
      "|                 10| 651.92|              21.99|  99.96|\n",
      "|                 12|1299.87|             499.95|   null|\n",
      "|                 12|1299.87|             299.98| 499.95|\n",
      "|                 12|1299.87|              250.0| 299.98|\n",
      "+-------------------+-------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using lagging \n",
    "spark.sql('select order_item_order_id,\\\n",
    "round(sum(order_item_subtotal) over(partition by order_item_order_id), 2) as revenue,\\\n",
    "order_item_subtotal,\\\n",
    "lag(order_item_subtotal) over (partition by order_item_order_id order by order_item_subtotal desc) as lagging \\\n",
    "from order_items').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the order_date, product_id and revenue \n",
    "spark.sql('select order_date, order_item_product_id  as product_id,sum(order_item_subtotal) as revenue from orders join order_items \\\n",
    "on order_id = order_item_order_id  where order_status in (\"COMPLETE\",\"CLOSED\") group by order_date, order_item_product_id').registerTempTable('joinedtbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rank base on per day based on revneue\n",
    "PerDayrevenue = spark.sql('select order_date, product_id,round(revenue,2) as Revenue,\\\n",
    "rank() over(partition by order_date order by revenue desc) as rank from joinedtbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Data to different targets using API.\n",
    "Supported File format:- csv,text,json,orc,parquet, avro(3rd party)\n",
    "Data can be written in Hive table .\n",
    "Data can be written into Relation database by connection through JDBC/3rd party lib.\n",
    "\n",
    "************************\n",
    "Write to any RDBMS \n",
    "************************\n",
    "df.write.format('jdbc').option('url','jdbc://192.168.52.13:3306').\\\n",
    "option('dbtable','db1.table2').\\\n",
    "option('user', 'root').\\\n",
    "option('password','root').\n",
    "save(mode=append)\n",
    "\n",
    "or \n",
    "\n",
    "df.write.jdbc('jdbc://192.168.52.13:3306', 'db1.table2',mode=append,properties={'user':'root','password':'root'})\n",
    "\n",
    "************************\n",
    "Write to Hive\n",
    "************************\n",
    "\n",
    "df.write.saveAsTable(name, format=None, mode=None, partitionBy=None) # creates new table in hive\n",
    "\n",
    "dfwrite.insertInto(tableName, overwrite=False) \n",
    "#insert df into existing hive table defualy append.\n",
    "\n",
    "df.write.bucketBy(numBuckets, col, *cols) \n",
    "#Buckets the output by the given columns.If specified,the output is laid out on the file system similar to Hive's bucketing scheme.\n",
    "\n",
    "df.write.partitionBy(*cols)  \n",
    "#Partitions the output by the given columns on the file system.\n",
    "                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerDayrevenue.write.options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------+---+\n",
      "|                _c0| _c1|    _c2|_c3|\n",
      "+-------------------+----+-------+---+\n",
      "|2013-07-25 00:00:00|1004|5599.72|  1|\n",
      "|2013-07-25 00:00:00| 191|5099.49|  2|\n",
      "|2013-07-25 00:00:00| 957| 4499.7|  3|\n",
      "|2013-07-25 00:00:00| 365|3359.44|  4|\n",
      "|2013-07-25 00:00:00|1073|2999.85|  5|\n",
      "|2013-07-25 00:00:00|1014|2798.88|  6|\n",
      "|2013-07-25 00:00:00| 403|1949.85|  7|\n",
      "|2013-07-25 00:00:00| 502| 1650.0|  8|\n",
      "|2013-07-25 00:00:00| 627|1079.73|  9|\n",
      "|2013-07-25 00:00:00| 226| 599.99| 10|\n",
      "|2013-07-25 00:00:00|  24| 319.96| 11|\n",
      "|2013-07-25 00:00:00| 821| 207.96| 12|\n",
      "|2013-07-25 00:00:00| 625| 199.99| 13|\n",
      "|2013-07-25 00:00:00| 705| 119.99| 14|\n",
      "|2013-07-25 00:00:00| 572| 119.97| 15|\n",
      "|2013-07-25 00:00:00| 666| 109.99| 16|\n",
      "|2013-07-25 00:00:00| 725|  108.0| 17|\n",
      "|2013-07-25 00:00:00| 134|  100.0| 18|\n",
      "|2013-07-25 00:00:00| 906|  99.96| 19|\n",
      "|2013-07-25 00:00:00| 828|  95.97| 20|\n",
      "+-------------------+----+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/hduser/Documents/Compressedoutput already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2523.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/hduser/Documents/Compressedoutput already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-aac2c7abf67d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#spark.read.table(tableName) reads table from hive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#PerDayrevenue.write.insertInto()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mPerDayrevenue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'codec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/hduser/Documents/Compressedoutput'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#save the output in compressed form in json format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/hduser/Documents/Compressedoutput already exists.;'"
     ]
    }
   ],
   "source": [
    "#Save the output in compressed form is csv format\n",
    "spark.read.csv('/home/hduser/Documents/Compressedoutput').show()\n",
    "\n",
    "#(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None,\\\n",
    "\n",
    "#inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, \n",
    "\n",
    "#positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,\n",
    "\n",
    "#maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None,\n",
    "\n",
    "#multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)\n",
    "\n",
    "#spark.read.load()\n",
    "\n",
    "#spark.read.load(path=None, format=None, schema=None, **options)\n",
    "\n",
    "#spark.read.option(key, value)\n",
    "\n",
    "#spark.read.text() each line in the text file is a new row in the resulting DataFrame.\n",
    "\n",
    "#spark.read.table(tableName) reads table from hive \n",
    "#PerDayrevenue.write.insertInto() \n",
    "PerDayrevenue.write.format('csv').option('codec','gzip').save('/home/hduser/Documents/Compressedoutput')\n",
    "\n",
    "#save the output in compressed form in json format\n",
    "#PerDayrevenue.write.option('compression','gzip').format('json').save('/home/hduser/Documents/CompressedoutputJson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time configuration changing compressed algo for parquet file format\n",
    "spark.conf.set('spark.sql.parquet.compression.codec','gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerDayrevenue.write.parquet('/home/hduser/Documents/CompressedoutputParque')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+----+\n",
      "|         order_date|product_id|Revenue|rank|\n",
      "+-------------------+----------+-------+----+\n",
      "|2013-07-25 00:00:00|      1004|5599.72|   1|\n",
      "|2013-07-25 00:00:00|       191|5099.49|   2|\n",
      "|2013-07-25 00:00:00|       957| 4499.7|   3|\n",
      "|2013-07-25 00:00:00|       365|3359.44|   4|\n",
      "|2013-07-25 00:00:00|      1073|2999.85|   5|\n",
      "|2013-07-25 00:00:00|      1014|2798.88|   6|\n",
      "|2013-07-25 00:00:00|       403|1949.85|   7|\n",
      "|2013-07-25 00:00:00|       502| 1650.0|   8|\n",
      "|2013-07-25 00:00:00|       627|1079.73|   9|\n",
      "|2013-07-25 00:00:00|       226| 599.99|  10|\n",
      "|2013-07-25 00:00:00|        24| 319.96|  11|\n",
      "|2013-07-25 00:00:00|       821| 207.96|  12|\n",
      "|2013-07-25 00:00:00|       625| 199.99|  13|\n",
      "|2013-07-25 00:00:00|       705| 119.99|  14|\n",
      "|2013-07-25 00:00:00|       572| 119.97|  15|\n",
      "|2013-07-25 00:00:00|       666| 109.99|  16|\n",
      "|2013-07-25 00:00:00|       725|  108.0|  17|\n",
      "|2013-07-25 00:00:00|       134|  100.0|  18|\n",
      "|2013-07-25 00:00:00|       906|  99.96|  19|\n",
      "|2013-07-25 00:00:00|       828|  95.97|  20|\n",
      "+-------------------+----------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how to read the files with speecific file format\n",
    "#spark.read.parquet('/home/hduser/Documents/CompressedoutputParque').show()\n",
    "#spark.read.format('parquet').load('/home/hduser/Documents/CompressedoutputParque').show()\n",
    "spark.read.csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------+---+\n",
      "|                _c0| _c1|    _c2|_c3|\n",
      "+-------------------+----+-------+---+\n",
      "|2013-07-25 00:00:00|1004|5599.72|  1|\n",
      "|2013-07-25 00:00:00| 191|5099.49|  2|\n",
      "|2013-07-25 00:00:00| 957| 4499.7|  3|\n",
      "|2013-07-25 00:00:00| 365|3359.44|  4|\n",
      "|2013-07-25 00:00:00|1073|2999.85|  5|\n",
      "|2013-07-25 00:00:00|1014|2798.88|  6|\n",
      "|2013-07-25 00:00:00| 403|1949.85|  7|\n",
      "|2013-07-25 00:00:00| 502| 1650.0|  8|\n",
      "|2013-07-25 00:00:00| 627|1079.73|  9|\n",
      "|2013-07-25 00:00:00| 226| 599.99| 10|\n",
      "|2013-07-25 00:00:00|  24| 319.96| 11|\n",
      "|2013-07-25 00:00:00| 821| 207.96| 12|\n",
      "|2013-07-25 00:00:00| 625| 199.99| 13|\n",
      "|2013-07-25 00:00:00| 705| 119.99| 14|\n",
      "|2013-07-25 00:00:00| 572| 119.97| 15|\n",
      "|2013-07-25 00:00:00| 666| 109.99| 16|\n",
      "|2013-07-25 00:00:00| 725|  108.0| 17|\n",
      "|2013-07-25 00:00:00| 134|  100.0| 18|\n",
      "|2013-07-25 00:00:00| 906|  99.96| 19|\n",
      "|2013-07-25 00:00:00| 828|  95.97| 20|\n",
      "+-------------------+----+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/home/hduser/Documents/Compressedoutput').show()\n",
    "#(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None,\\\n",
    "#inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, \n",
    "#positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,\n",
    "#maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None,\n",
    "#multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.load()\n",
    "#spark.read.load(path=None, format=None, schema=None, **options)\n",
    "#spark.read.option(key, value)\n",
    "#spark.read.text() each line in the text file is a new row in the resulting DataFrame.\n",
    "#spark.read.table(tableName) reads table from hive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
