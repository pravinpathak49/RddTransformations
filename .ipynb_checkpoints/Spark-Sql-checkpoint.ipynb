{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark_SQL').config('spark.sql.shuffle.partitions',2).getOrCreate()\n",
    "#read the csv file in dataframe\n",
    "OrderSchema = StructType([StructField(\"order_id\", IntegerType()),StructField(\"order_date\", StringType()), StructField(\"customer_id\", IntegerType()),StructField(\"order_status\", StringType())])\n",
    "orderDF = spark.read.csv(path='/home/hduser/Documents/RddTransformations/Data/orders.csv', sep=\",\", schema=OrderSchema)\n",
    "\n",
    "orderItemschema = StructType([StructField(\"order_item_id\", IntegerType()),StructField(\"order_item_order_id\", IntegerType()),StructField(\"order_item_product_id\", IntegerType()),StructField(\"order_item_quantity\", IntegerType()),StructField(\"order_item_subtotal\", FloatType()), StructField(\"order_item_product_price\", FloatType())])\n",
    "orderItemsDF = spark.read.csv(path='/home/hduser/Documents/RddTransformations/Data/order_items.csv', sep=\",\", schema=orderItemschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF.registerTempTable('orders')\n",
    "orderItemsDF.registerTempTable('order_items')\n",
    "\n",
    "#to Drop temp table\n",
    "#spark.catalog.dropTempView('order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |order_items|       true|\n",
      "|        |     orders|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql whatever it executes it returns DataFrame\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from orders').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the order_items whose order_status is complete or closed\n",
    "#1. using in operator\n",
    " spark.sql('select * from order_items where order_item_order_id in \\\n",
    " (select order_id from orders where order_status in (\"COMPLETE\",\"CLOSED\"))').show(5)\n",
    "#2 Using Join \n",
    "#spark.sql('select * from order_items join orders on \\\n",
    "#order_items.order_item_order_id = orders.order_id where orders.order_status in (\"COMPLETE\",\"CLOSED\")').show()\n",
    "\n",
    "#3 Using ASCII standard sql code (suggested)\n",
    "#spark.sql('select * from order_items, orders where order_items.order_item_order_id = orders.order_id \\\n",
    "#and orders.order_status in (\"COMPLETE\",\"CLOSED\")').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11452"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show all the orders whose order_id  is not in order_items\n",
    "#spark.sql('select * from orders where order_id not in (select order_item_order_id from order_items)').count()\n",
    "#11452\n",
    "#using Join\n",
    "spark.sql('select * from orders left outer join order_items on\\\n",
    "    orders.order_id = order_items.order_item_order_id where order_items.order_item_order_id is null').count()\n",
    "#11452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+------------------------+\n",
      "|         order_date|order_item_product_id|sum(order_item_subtotal)|\n",
      "+-------------------+---------------------+------------------------+\n",
      "|2013-07-25 00:00:00|                 1014|       6397.439922332764|\n",
      "|2013-07-25 00:00:00|                  926|       79.94999694824219|\n",
      "|2013-07-25 00:00:00|                  191|        8499.15005493164|\n",
      "|2013-07-25 00:00:00|                  134|                   100.0|\n",
      "|2013-07-25 00:00:00|                  276|       255.9199981689453|\n",
      "+-------------------+---------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the revenue per day per product item\n",
    "spark.sql('select order_date, order_item_product_id,sum(order_item_subtotal) from orders join order_items on \\\n",
    "          orders.order_id = order_items.order_item_order_id group by order_date,order_item_product_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('select order_date, order_item_product_id,sum(order_item_subtotal) from orders join order_items on \\\n",
    "#          orders.order_id = order_items.order_item_order_id group by order_date,order_item_product_id')\\\n",
    "#          .write.csv('/home/hduser/Documents/RddTransformations/sparksql/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_subtotal)|\n",
      "+-------------------+------------------------+\n",
      "|                  2|       579.9800109863281|\n",
      "|                  4|       699.8500099182129|\n",
      "|                  5|      1129.8600387573242|\n",
      "|                 10|        651.920015335083|\n",
      "|                 12|      1299.8700256347656|\n",
      "+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using group by functions. We can have those columns in select by clause which we are doing group by.\n",
    "spark.sql('select order_item_order_id, sum(order_item_subtotal) from order_items\\\n",
    "           group by order_item_order_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|                  1|                  957|             299.98|299.9800109863281|\n",
      "|                  2|                 1073|             199.99|579.9800109863281|\n",
      "|                  2|                  502|              250.0|579.9800109863281|\n",
      "|                  2|                  403|             129.99|579.9800109863281|\n",
      "|                  4|                  897|              49.98|699.8500099182129|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if you need more columns you need to self join and in this situation partition by comes\n",
    "spark.sql('select ot.order_item_order_id,ot.order_item_product_id ,ot.order_item_subtotal,o.revenue \\\n",
    "           from order_items ot join \\\n",
    "          (select order_item_order_id, sum(order_item_subtotal)as revenue \\\n",
    "          from order_items group by order_item_order_id)o\\\n",
    "          on ot.order_item_order_id = o.order_item_order_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "|                  2|                 1073|             199.99|579.9800109863281|\n",
      "|                  2|                  502|              250.0|579.9800109863281|\n",
      "|                  2|                  403|             129.99|579.9800109863281|\n",
      "|                  4|                  897|              49.98|699.8500099182129|\n",
      "|                  4|                  365|             299.95|699.8500099182129|\n",
      "+-------------------+---------------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using partition By we can get multiple columns value as well. Not needed to join and exeution is also fast\n",
    "#get the total revenue as per product id.\n",
    "spark.sql('select order_item_order_id,order_item_product_id ,order_item_subtotal ,\\\n",
    "sum(order_item_subtotal) over (partition by order_item_order_id)as revenue from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|Percentage|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "|                  2|                 1073|             199.99|579.9800109863281|     34.48|\n",
      "|                  2|                  502|              250.0|579.9800109863281|      43.1|\n",
      "|                  2|                  403|             129.99|579.9800109863281|     22.41|\n",
      "|                  4|                  897|              49.98|699.8500099182129|      7.14|\n",
      "|                  4|                  365|             299.95|699.8500099182129|     42.86|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the % of each product id\n",
    "spark.sql('select order_item_order_id,order_item_product_id ,order_item_subtotal ,\\\n",
    "sum(order_item_subtotal) over (partition by order_item_order_id)as revenue,\\\n",
    "round(order_item_subtotal/(sum(order_item_subtotal) over (partition by order_item_order_id))* 100 ,2)\\\n",
    "as Percentage from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "|order_item_order_id|order_item_product_id|order_item_subtotal|          revenue|Percentage|rank|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "|                  2|                  403|             129.99|579.9800109863281|      0.22|   1|\n",
      "|                  2|                 1073|             199.99|579.9800109863281|      0.34|   2|\n",
      "|                  2|                  502|              250.0|579.9800109863281|      0.43|   3|\n",
      "|                  4|                  897|              49.98|699.8500099182129|      0.07|   1|\n",
      "|                  4|                  502|              150.0|699.8500099182129|      0.21|   2|\n",
      "|                  4|                 1014|             199.92|699.8500099182129|      0.29|   3|\n",
      "|                  4|                  365|             299.95|699.8500099182129|      0.43|   4|\n",
      "+-------------------+---------------------+-------------------+-----------------+----------+----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank the records based on order_item_subtotal of each order_id.\n",
    "#these analytical functions can be used in select clause.\n",
    "#execution of sql happens like\n",
    "#from --> where and/or join --> group by --> having --> select --> order by\n",
    "spark.sql('select order_item_order_id,\\\n",
    "order_item_product_id,\\\n",
    "order_item_subtotal,\\\n",
    "sum(order_item_subtotal) over(partition by order_item_order_id) as revenue,\\\n",
    "round(order_item_subtotal/sum(order_item_subtotal) over(partition by order_item_order_id), 2)as Percentage, \\\n",
    "rank() over (partition by order_item_order_id order by order_item_subtotal) as rank \\\n",
    "from order_items').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+-------+\n",
      "|order_item_order_id|order_item_subtotal|revenue|ranking|\n",
      "+-------------------+-------------------+-------+-------+\n",
      "|                  2|             129.99| 579.98|      1|\n",
      "|                  2|             199.99| 579.98|      2|\n",
      "|                  4|              49.98| 699.85|      1|\n",
      "|                  4|              150.0| 699.85|      2|\n",
      "|                  5|              99.96|1129.86|      1|\n",
      "|                  5|             129.99|1129.86|      2|\n",
      "|                 10|              21.99| 651.92|      1|\n",
      "|                 10|              99.96| 651.92|      2|\n",
      "|                 12|              100.0|1299.87|      1|\n",
      "|                 12|             149.94|1299.87|      2|\n",
      "|                 13|             127.96| 127.96|      1|\n",
      "|                 14|               50.0| 549.94|      1|\n",
      "|                 14|              99.96| 549.94|      2|\n",
      "|                 18|             119.98| 449.96|      1|\n",
      "|                 18|             129.99| 449.96|      2|\n",
      "|                 25|             399.98| 399.98|      1|\n",
      "|                 28|              59.99| 1159.9|      1|\n",
      "|                 28|              99.99| 1159.9|      2|\n",
      "|                 31|             499.95| 499.95|      1|\n",
      "|                 36|              100.0| 799.96|      1|\n",
      "+-------------------+-------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get top 2 items of each order_id\n",
    "spark.sql('select  *from (select \\\n",
    "order_item_order_id, \\\n",
    "order_item_subtotal, \\\n",
    "round(sum(order_item_subtotal) over (partition by order_item_order_id), 2) as revenue, \\\n",
    "rank() over(partition by order_item_order_id order by order_item_subtotal) as ranking \\\n",
    "from order_items ) where ranking < 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-------------------+-------+\n",
      "|order_item_order_id|          revenue|order_item_subtotal|leading|\n",
      "+-------------------+-----------------+-------------------+-------+\n",
      "|                  2|579.9800109863281|              250.0| 199.99|\n",
      "|                  2|579.9800109863281|             199.99| 129.99|\n",
      "|                  2|579.9800109863281|             129.99|   null|\n",
      "|                  4|699.8500099182129|             299.95| 199.92|\n",
      "|                  4|699.8500099182129|             199.92|  150.0|\n",
      "+-------------------+-----------------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we use lead() and lag() to get the next or previous item for the column specified in the functions.\n",
    "spark.sql('select order_item_order_id,\\\n",
    "sum(order_item_subtotal) over(partition by order_item_order_id) as revenue,\\\n",
    "order_item_subtotal,\\\n",
    "lead(order_item_subtotal) over (partition by order_item_order_id order by order_item_subtotal desc) as leading \\\n",
    "from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------------+-------+\n",
      "|order_item_order_id|revenue|order_item_subtotal|lagging|\n",
      "+-------------------+-------+-------------------+-------+\n",
      "|                  2| 579.98|              250.0|   null|\n",
      "|                  2| 579.98|             199.99|  250.0|\n",
      "|                  2| 579.98|             129.99| 199.99|\n",
      "|                  4| 699.85|             299.95|   null|\n",
      "|                  4| 699.85|             199.92| 299.95|\n",
      "|                  4| 699.85|              150.0| 199.92|\n",
      "|                  4| 699.85|              49.98|  150.0|\n",
      "|                  5|1129.86|             299.98|   null|\n",
      "|                  5|1129.86|             299.98| 299.98|\n",
      "|                  5|1129.86|             299.95| 299.98|\n",
      "|                  5|1129.86|             129.99| 299.95|\n",
      "|                  5|1129.86|              99.96| 129.99|\n",
      "|                 10| 651.92|             199.99|   null|\n",
      "|                 10| 651.92|             199.99| 199.99|\n",
      "|                 10| 651.92|             129.99| 199.99|\n",
      "|                 10| 651.92|              99.96| 129.99|\n",
      "|                 10| 651.92|              21.99|  99.96|\n",
      "|                 12|1299.87|             499.95|   null|\n",
      "|                 12|1299.87|             299.98| 499.95|\n",
      "|                 12|1299.87|              250.0| 299.98|\n",
      "+-------------------+-------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using lagging \n",
    "spark.sql('select order_item_order_id,\\\n",
    "round(sum(order_item_subtotal) over(partition by order_item_order_id), 2) as revenue,\\\n",
    "order_item_subtotal,\\\n",
    "lag(order_item_subtotal) over (partition by order_item_order_id order by order_item_subtotal desc) as lagging \\\n",
    "from order_items').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from order_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+------------------+----+\n",
      "|order_id|order_item_subtotal|         order_date|      Dailyrevenue|rank|\n",
      "+--------+-------------------+-------------------+------------------+----+\n",
      "|    7507|              34.99|2013-09-09 00:00:00|124454.49238586426|   1|\n",
      "|    9837|              34.99|2013-09-24 00:00:00|126898.56241798401|   1|\n",
      "|    9866|              34.99|2013-09-24 00:00:00|126898.56241798401|   1|\n",
      "|    9985|              34.99|2013-09-25 00:00:00|141775.64274215698|   1|\n",
      "|   14074|              34.99|2013-10-21 00:00:00| 53567.91116333008|   1|\n",
      "|   14120|              34.99|2013-10-21 00:00:00| 53567.91116333008|   1|\n",
      "|   21855|              34.99|2013-12-06 00:00:00|127911.60246276855|   1|\n",
      "|   22380|              34.99|2013-12-09 00:00:00| 73927.68151283264|   1|\n",
      "|   32076|              34.99|2014-02-08 00:00:00| 82435.65173149109|   1|\n",
      "|   33848|              34.99|2014-02-18 00:00:00|104887.89197158813|   1|\n",
      "|   35570|              34.99|2014-03-01 00:00:00|125918.40251731873|   1|\n",
      "|   38180|              34.99|2014-03-16 00:00:00| 93206.49175453186|   1|\n",
      "|   38183|              34.99|2014-03-16 00:00:00| 93206.49175453186|   1|\n",
      "|   38185|              34.99|2014-03-16 00:00:00| 93206.49175453186|   1|\n",
      "|   39190|              34.99|2014-03-23 00:00:00|103857.59194755554|   1|\n",
      "|   39751|              34.99|2014-03-27 00:00:00| 68161.77126121521|   1|\n",
      "|   41968|              34.99|2014-04-09 00:00:00|125304.02256202698|   1|\n",
      "|   45350|              34.99|2014-05-02 00:00:00|102048.32214736938|   1|\n",
      "|   53364|              34.99|2014-06-26 00:00:00|  88760.4815940857|   1|\n",
      "|   53499|              34.99|2014-06-27 00:00:00|139952.39247322083|   1|\n",
      "+--------+-------------------+-------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get Daily top N product per day.\n",
    "spark.sql('select orders.order_id,\\\n",
    "order_items.order_item_subtotal, \\\n",
    "orders.order_date, \\\n",
    "sum (order_items.order_item_subtotal) over (partition by orders.order_date) as Dailyrevenue,\\\n",
    "rank() over (partition by order_items.order_item_product_id order by order_items.order_item_subtotal) as rank \\\n",
    "from orders join order_items on orders.order_id = order_items.order_item_order_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
